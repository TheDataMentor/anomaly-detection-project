{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Anomaly Detection using ARIMA Forecasting Residuals\n",
    "\n",
    "## Project Description\n",
    "\n",
    "This notebook demonstrates the use of ARIMA (AutoRegressive Integrated Moving Average) forecasting residuals to detect anomalies in time series data. This method was successfully applied in a real-world scenario for a client, where it uncovered previously undetected anomalies in their business data. This notebook recreates a similar analysis using synthetic data.\n",
    "\n",
    "## Problem\n",
    "\n",
    "Traditional anomaly detection methods often fail to identify subtle irregularities in time series data, especially when dealing with complex patterns and seasonality. This can lead to missed opportunities for optimization or undetected issues in business processes.\n",
    "\n",
    "## Solution\n",
    "\n",
    "We implement an anomaly detection system using ARIMA forecasting residuals. This approach allows us to:\n",
    "\n",
    "1. Model the time series data, accounting for trends and seasonality.\n",
    "2. Generate forecasts based on the ARIMA model.\n",
    "3. Calculate residuals (differences between actual and forecasted values).\n",
    "4. Identify anomalies by analyzing the distribution of these residuals.\n",
    "\n",
    "## Real-World Application\n",
    "\n",
    "This method was applied for a retail client who suspected inconsistencies in their sales data but couldn't pinpoint the issues. By applying this ARIMA-based anomaly detection:\n",
    "\n",
    "1. We identified several days with anomalous sales patterns that weren't flagged by their existing systems.\n",
    "2. Upon investigation, these anomalies corresponded to:\n",
    "   - Unlogged promotional events\n",
    "   - Inventory system glitches\n",
    "   - Unreported regional events affecting foot traffic\n",
    "3. The client was able to correct historical data and implement new tracking measures.\n",
    "4. This led to more accurate forecasting and improved operational efficiency.\n",
    "\n",
    "In this notebook, we'll recreate a similar analysis using synthetic data to demonstrate the process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from statsmodels.tsa.arima.model import ARIMA\n",
    "from statsmodels.tsa.seasonal import seasonal_decompose\n",
    "from statsmodels.graphics.tsaplots import plot_acf, plot_pacf\n",
    "\n",
    "%matplotlib inline\n",
    "plt.style.use('seaborn')\n",
    "sns.set_context(\"notebook\", font_scale=1.5, rc={\"lines.linewidth\": 2.5})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Generation\n",
    "\n",
    "First, let's implement a function to generate synthetic retail purchase data. This data will mimic the characteristics of real-world retail data, including trends, seasonality, and occasional anomalies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_synthetic_data(start_date='2022-01-01', periods=730, freq='D'):\n",
    "    \"\"\"\n",
    "    Generate synthetic retail purchase data with trend, seasonality, and anomalies for multiple stores and categories.\n",
    "    \"\"\"\n",
    "    date_range = pd.date_range(start=start_date, periods=periods, freq=freq)\n",
    "    \n",
    "    # Define categories and stores\n",
    "    categories = ['Electronics', 'Clothing', 'Groceries', 'Home & Garden', 'Toys']\n",
    "    stores = [f'Store_{i}' for i in range(1, 11)]  # 10 stores\n",
    "    \n",
    "    data = []\n",
    "    for store in stores:\n",
    "        for category in categories:\n",
    "            # Generate base trend\n",
    "            base_value = np.random.randint(100, 200)\n",
    "            trend = np.linspace(base_value, base_value * 2, periods)\n",
    "            \n",
    "            # Add seasonality\n",
    "            seasonality_amplitude = np.random.uniform(30, 70)\n",
    "            seasonality = seasonality_amplitude * np.sin(np.arange(periods) * (2 * np.pi / 365))\n",
    "            \n",
    "            # Add noise\n",
    "            noise = np.random.normal(0, base_value * 0.1, periods)\n",
    "            \n",
    "            # Combine components\n",
    "            values = trend + seasonality + noise\n",
    "            \n",
    "            # Add anomalies (randomly for each store-category combination)\n",
    "            anomaly_indices = np.random.choice(periods, size=3, replace=False)\n",
    "            for idx in anomaly_indices:\n",
    "                values[idx] += np.random.uniform(base_value * 0.5, base_value)\n",
    "            \n",
    "            # Ensure no negative values\n",
    "            values = np.maximum(values, 0)\n",
    "            \n",
    "            for date, value in zip(date_range, values):\n",
    "                data.append({\n",
    "                    'date': date,\n",
    "                    'store': store,\n",
    "                    'category': category,\n",
    "                    'value': round(value, 2)\n",
    "                })\n",
    "    \n",
    "    df = pd.DataFrame(data)\n",
    "    return df\n",
    "\n",
    "# Generate the synthetic data\n",
    "df = generate_synthetic_data()\n",
    "print(df.head())\n",
    "print(f\"\\nDataset shape: {df.shape}\")\n",
    "print(f\"\\nDate range: {df['date'].min()} to {df['date'].max()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Exploration\n",
    "\n",
    "Let's explore our synthetic data to understand its characteristics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Aggregate data by date\n",
    "df_agg = df.groupby('date')['value'].sum().reset_index()\n",
    "df_agg.set_index('date', inplace=True)\n",
    "\n",
    "# Plot the aggregated time series\n",
    "plt.figure(figsize=(15, 7))\n",
    "plt.plot(df_agg.index, df_agg['value'])\n",
    "plt.title('Aggregated Sales Over Time')\n",
    "plt.xlabel('Date')\n",
    "plt.ylabel('Total Sales')\n",
    "plt.show()\n",
    "\n",
    "# Display summary statistics\n",
    "print(df_agg['value'].describe())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Time Series Decomposition\n",
    "\n",
    "To better understand the components of our time series, let's decompose it into trend, seasonality, and residual components."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform time series decomposition\n",
    "result = seasonal_decompose(df_agg['value'], model='additive', period=365)\n",
    "\n",
    "# Plot the decomposition\n",
    "fig, (ax1, ax2, ax3, ax4) = plt.subplots(4, 1, figsize=(15, 20))\n",
    "result.observed.plot(ax=ax1)\n",
    "ax1.set_title('Observed')\n",
    "result.trend.plot(ax=ax2)\n",
    "ax2.set_title('Trend')\n",
    "result.seasonal.plot(ax=ax3)\n",
    "ax3.set_title('Seasonal')\n",
    "result.resid.plot(ax=ax4)\n",
    "ax4.set_title('Residual')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ARIMA Modeling\n",
    "\n",
    "Now, let's implement the ARIMA modeling and anomaly detection function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def detect_anomalies(df, value_col='value', date_col='date', window=30, threshold=3):\n",
    "    \"\"\"\n",
    "    Detect anomalies in time series data using ARIMA forecasting residuals.\n",
    "    \n",
    "    Args:\n",
    "    df (pd.DataFrame): Input dataframe with datetime index and value column\n",
    "    value_col (str): Name of the column containing the time series values\n",
    "    date_col (str): Name of the column containing the dates (if not index)\n",
    "    window (int): Rolling window size for calculating mean and std of residuals\n",
    "    threshold (float): Number of standard deviations to use as anomaly threshold\n",
    "    \n",
    "    Returns:\n",
    "    pd.DataFrame: Original dataframe with additional columns for forecast and anomaly flag\n",
    "    \"\"\"\n",
    "    \n",
    "    # Ensure the dataframe is sorted by date\n",
    "    df = df.sort_index() if df.index.name == date_col else df.sort_values(date_col)\n",
    "    \n",
    "    # Fit ARIMA model\n",
    "    model = ARIMA(df[value_col], order=(1,1,1))  # You might need to adjust these parameters\n",
    "    results = model.fit()\n",
    "    \n",
    "    # Generate forecasts\n",
    "    forecast = results.forecast(steps=len(df))\n",
    "    \n",
    "    # Calculate residuals\n",
    "    df['forecast'] = forecast\n",
    "    df['residual'] = df[value_col] - df['forecast']\n",
    "    \n",
    "    # Calculate rolling statistics\n",
    "    df['residual_mean'] = df['residual'].rolling(window=window).mean()\n",
    "    df['residual_std'] = df['residual'].rolling(window=window).std()\n",
    "    \n",
    "    # Detect anomalies\n",
    "    df['anomaly'] = abs(df['residual'] - df['residual_mean']) > (threshold * df['residual_std'])\n",
    "    \n",
    "    return df\n",
    "\n",
    "# Apply anomaly detection\n",
    "df_with_anomalies = detect_anomalies(df_agg)\n",
    "print(f\"Detected {df_with_anomalies['anomaly'].sum()} anomalies\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualizing Anomalies\n",
    "\n",
    "Let's visualize our time series data with the detected anomalies highlighted."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_time_series_with_anomalies(df, value_col='value', date_col='date', anomaly_col='anomaly'):\n",
    "    plt.figure(figsize=(15, 7))\n",
    "    plt.plot(df.index, df[value_col], label='Original Data')\n",
    "    plt.plot(df.index, df['forecast'], color='red', alpha=0.5, label='Forecast')\n",
    "    \n",
    "    anomalies = df[df[anomaly_col]]\n",
    "    plt.scatter(anomalies.index, anomalies[value_col], color='green', label='Anomalies', s=100)\n",
    "    \n",
    "    plt.title('Time Series with Detected Anomalies')\n",
    "    plt.xlabel('Date')\n",
    "    plt.ylabel('Value')\n",
    "    plt.legend()\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "plot_time_series_with_anomalies(df_with_anomalies)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analyzing Anomalies\n",
    "\n",
    "Let's take a closer look at the detected anomalies and their characteristics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "anomalies = df_with_anomalies[df_with_anomalies['anomaly']]\n",
    "print(\"Anomaly details:\")\n",
    "print(anomalies[['value', 'forecast', 'residual']])\n",
    "\n",
    "# Calculate and print the percentage difference from forecast for each anomaly\n",
    "anomalies['percent_diff'] = (anomalies['value'] - anomalies['forecast']) / anomalies['forecast'] * 100\n",
    "print(\"\\nPercentage difference from forecast:\")\n",
    "print(anomalies['percent_diff'])\n",
    "\n",
